{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Udacity Reinforcement Learning - Project 3: Tennis\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The purpose of this project is to implement and train deep reinforcement learning agents that can play a ball to each other across a tennis court and in doing so develop further knowledge and experience of using multi agent architectures.\n",
    "\n",
    "## Problem Domain\n",
    "\n",
    "### Environement \n",
    "\n",
    "The environment consists of a tennis court on which 2 agents must play a ball over the net as many times as possible without playing the ball out of bounds and without the ball hitting the net or ground. The episosde will terminate if either of these occur.\n",
    "\n",
    "### Reward and Goal\n",
    "\n",
    "Rewards of +0.1 will be granted to an agent when it manages to successfully play a the ball over the net. If the ball is played out of bounds or hits the net or ground a reward of -0.01 will be given to that agent.\n",
    "\n",
    "The problem will be considered complete when the maximum score between the 2 agents averages above +0.5 for the previous 100 episodes.\n",
    "\n",
    "\n",
    "### State and Action Spaces\n",
    "\n",
    "Each agent can make actions of size 2, corresponding to up/down and left/right.\n",
    "\n",
    "The state space has 24 dimensions, with each agent making its own observations at each step. \n",
    "\n",
    "\n",
    "## Solution Implementation\n",
    "\n",
    "The solution consists of 4 key components:\n",
    "\n",
    "### Unity Environment\n",
    "\n",
    "The Unity environment is responsible for managing the simulated world, determining the agents next state and reward for a given action.\n",
    "\n",
    "### Tennis.ipynb\n",
    "\n",
    "This notebook manages the high level orchestration of the training loop and episodes and visualises the results.\n",
    "\n",
    "### ddpg_agent.py\n",
    "\n",
    "ddpg_agent.py implements the ReplayBuffer, OUNoise and Agent classes which together conduct the running of the DDPG Network that is used to drive both of the agents in the environment.\n",
    "\n",
    "### model.py\n",
    "\n",
    "This implements the simple Feed Forward Neural Network architectures for both the Actor and the Critic used by the DDPG agent. The Actor architecture implemented here consisted of a 3 layer network with Relu activation and hidden dimensions of 400 and 300 nodes respectively, the last layer has a tanh activation as opposed to Relu. Input and output dimensions correspond to the state and action spaces. The Critic architecture is also a 3 layer network with basic hidden dimensions of 400 and 300, however the action vector is also concatenated onto the second hidden layer. Input dimensions are that of the state space and the network outputs a single value.\n",
    "\n",
    "\n",
    "## Results\n",
    "\n",
    "The environment was considered solved for the 73 episodes between 8,977 and 9,050. A maximum average score of 0.64 was achieved after approximately 5 hours of training time.\n",
    "\n",
    "On episode 8,942 a maximum score of 2.7 was reached by one of the agents during a rally containing 53 successful shots and 1,000 actions taken by each agent.\n",
    "\n",
    "For plots of these results please see tennis.ipynb\n",
    "\n",
    "\n",
    "## Further Improvements\n",
    "\n",
    "\n",
    "There are a number of further steps that can be taken to develop the sophistication of the agent in addition to improving its performance, these include:\n",
    "\n",
    "- Refinement of network architecture and hyperparameters\n",
    "- Inclusion of a monte carlo tree search mechanism\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
